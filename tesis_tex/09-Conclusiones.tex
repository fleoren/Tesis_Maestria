\chapter{Conclusiones}

Vale la pena revisitar la hip\'otesis con la que se comenz\'o este trabajo:\\

\textit{`Bajo este nuevo conjunto de supuestos, es posible encontrar las estrategias \'optimas para todos los agentes tomadores de decisiones en el Juego de la Distribuci\'on de Cerveza, por medio de algoritmos de aprendizaje reforzado, que produzcan resultados en un tiempo suficientemente veloz como para poder accionarlas.'}

(Hay que escribir esto mejor, pero...)

* Vimos que efectivamente la estrategia aprendida con policy iteration le gana a la estrategia "obvia" constante de demanda promedio
* Vimos tambien que para cualquier agente es preferible seguir la estrategia de aprendizaje, sin importat lo que los demas agentes hagan: la utilidad total es mayor si todos se portan bien, pero no es necesario que todos se porten bien para que un solo agente mejore sus decisiones y su utilidad
* Es un algoritmo que converge relativamente rapido, asi que se puede ajustar rapidamente a cambios en la demanda: sin problema podria reentrenarse diario 



\section{Trabajo futuro}

La parte de Q learning que no me convergio?
Generalizar a un numero no fijo de agentes?

